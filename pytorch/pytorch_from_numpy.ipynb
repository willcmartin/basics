{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a347334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: implement _very_ basic functionalities of pytorch from scratch (but I'm going to cheat and use numpy)\n",
    "\n",
    "# large inspiration from:\n",
    "# - https://github.com/karpathy/micrograd\n",
    "# - https://github.com/geohot/tinygrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91e21485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch \n",
    "# import torchvision\n",
    "# import torch.nn as nn\n",
    "import numpy as np\n",
    "# import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16154db2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9p/ysr3lnbs16z7wxxlztrm7z0m0000gn/T/ipykernel_62481/2237398906.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Create tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# pytorch\n",
    "\n",
    "# Create tensors.\n",
    "x = torch.tensor(15., requires_grad=True)\n",
    "print(x)\n",
    "w = torch.tensor(10., requires_grad=True)\n",
    "b = torch.tensor(900., requires_grad=True)\n",
    "\n",
    "# Build a computational graph.\n",
    "y = w * x + b * 100    # y = 2 * x + 3\n",
    "print(y)\n",
    "\n",
    "# Compute gradients.\n",
    "# Computes the sum of gradients of given tensors with respect to graph leaves.\n",
    "y.backward()\n",
    "\n",
    "# Print out the gradients.\n",
    "# Computes and returns the sum of gradients of outputs with respect to the inputs.\n",
    "print(x.grad)    # x.grad = 2 = dy/dx = w\n",
    "print(w.grad)    # w.grad = 1 = dy/dw = x\n",
    "print(b.grad)    # b.grad = 1 = dy/db = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1c9f46a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [15.]\n",
      "w: [10.]\n",
      "b: [900.]\n",
      "\n",
      "y: [1050.]\n",
      "<class '__main__.Add'>\n",
      "150.0\n",
      "[900.]\n",
      "<class '__main__.Multiply'>\n",
      "[10.]\n",
      "[15.]\n",
      "None\n",
      "None\n",
      "None\n",
      "[10.]\n",
      "[15.]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# not pytorch\n",
    "\n",
    "class Tensor:\n",
    "    def __init__(self, data, children=()):\n",
    "        self.data = np.array(data, dtype=np.float32)\n",
    "        self.children = children\n",
    "        self.grad = 1\n",
    "        self.op = None\n",
    "        \n",
    "    def __mul__(self, other):\n",
    "        op = Multiply\n",
    "        output = op.forward(self, other)\n",
    "        output.op = op\n",
    "        return output\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        op = Add\n",
    "        output = op.forward(self, other)\n",
    "        output.op = op\n",
    "        return output\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        op = Subtract\n",
    "        output = op.forward(self, other)\n",
    "        output.op = op\n",
    "        return output\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        op = Power\n",
    "        output = op.forward(self, other)\n",
    "        output.op = op\n",
    "        return output\n",
    "    \n",
    "    def mean(self):\n",
    "        op = Mean\n",
    "        output = op.forward(self)\n",
    "        output.op = op\n",
    "        return output\n",
    "\n",
    "    def backward(self):\n",
    "        print(self.op)\n",
    "        if self.op is not None:\n",
    "            children_grads = self.op.backward(self, *self.children)\n",
    "            for node, grad in zip(self.children, children_grads):\n",
    "                print(node.data)\n",
    "                node.grad = grad\n",
    "        for node in self.children:\n",
    "            node.backward()\n",
    "            \n",
    "\n",
    "class Multiply:\n",
    "    def forward(a, b):\n",
    "        return Tensor(np.matmul(a.data, b.data), [a, b])\n",
    "    \n",
    "    def backward(parent, a, b):\n",
    "        return [b.data * parent.grad, a.data * parent.grad]\n",
    "    \n",
    "class Add:\n",
    "    def forward(a, b):\n",
    "        return Tensor(np.add(a.data, b.data), [a, b])\n",
    "    \n",
    "    def backward(parent, a, b):\n",
    "        return [parent.grad, parent.grad]\n",
    "    \n",
    "class Subtract:\n",
    "    def forward(a, b):\n",
    "        return Tensor(np.subtract(a.data, b.data), [a, b])\n",
    "    \n",
    "    def backward(parent, a, b):\n",
    "        return [parent.grad, parent.grad]\n",
    "    \n",
    "class Power:\n",
    "    def forward(a, b):\n",
    "        return Tensor(np.power(a.data, b), [a])\n",
    "    \n",
    "    def backward(parent, a):\n",
    "        return [parent.grad]\n",
    "    \n",
    "class Mean:\n",
    "    def forward(a):\n",
    "        return Tensor(np.mean(a.data), [a])\n",
    "    \n",
    "    def backward(parent, a):\n",
    "        return [parent.grad]\n",
    "    \n",
    "        \n",
    "\n",
    "x = Tensor([15])\n",
    "w = Tensor([10])\n",
    "b = Tensor([900])\n",
    "print(f'x: {x.data}')\n",
    "print(f'w: {w.data}')\n",
    "print(f'b: {b.data}')\n",
    "print(f'')\n",
    "\n",
    "y = w * x + b\n",
    "print(f'y: {y.data}')\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(x.grad)\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "062ac64d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  [[-0.1367  0.0321]\n",
      " [ 0.4447 -0.2277]]\n",
      "b:  [-0.6949 -0.1522]\n",
      "pred linear:  [[-0.41233402 -0.2671844 ]\n",
      " [-0.37118456 -0.3133366 ]]\n",
      "loss:  1.0183916\n",
      "<class '__main__.Mean'>\n",
      "[[0.76731646 3.1043465 ]\n",
      " [0.19156778 0.01033544]]\n",
      "<class '__main__.Power'>\n",
      "[[-0.875966   -1.7619156 ]\n",
      " [ 0.43768457 -0.10166338]]\n",
      "<class '__main__.Subtract'>\n",
      "[[-1.2883 -2.0291]\n",
      " [ 0.0665 -0.415 ]]\n",
      "[[-0.41233402 -0.2671844 ]\n",
      " [-0.37118456 -0.3133366 ]]\n",
      "None\n",
      "<class '__main__.Add'>\n",
      "[[ 0.28256595 -0.11498442]\n",
      " [ 0.32371542 -0.16113663]]\n",
      "[-0.6949 -0.1522]\n",
      "<class '__main__.Multiply'>\n",
      "[[-0.7837  0.3945]\n",
      " [-0.1218  0.6905]]\n",
      "[[-0.1367  0.0321]\n",
      " [ 0.4447 -0.2277]]\n",
      "None\n",
      "None\n",
      "None\n",
      "dL/dw:  [[-0.7837  0.3945]\n",
      " [-0.1218  0.6905]]\n",
      "dL/db:  1\n"
     ]
    }
   ],
   "source": [
    "# TODO: make random\n",
    "x = Tensor([[-0.7837,  0.3945],\n",
    "            [-0.1218,  0.6905]])\n",
    "\n",
    "y = Tensor([[-1.2883, -2.0291],\n",
    "            [ 0.0665, -0.4150]])\n",
    "\n",
    "# nn building blocks\n",
    "# start with linear layer\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        # TODO: make random and transpose weight\n",
    "        self.weight = Tensor(np.transpose([[-0.1367,  0.4447], [ 0.0321, -0.2277]]))\n",
    "        self.bias = Tensor([-0.6949, -0.1522])\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return x * self.weight + self.bias\n",
    "    \n",
    "\n",
    "linear = Linear(2, 2)\n",
    "print ('w: ', linear.weight.data)\n",
    "print ('b: ', linear.bias.data)\n",
    "\n",
    "pred = linear(x)\n",
    "print(\"pred linear: \", pred.data)\n",
    "\n",
    "# loss function\n",
    "\n",
    "class MSELoss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, pred, true):\n",
    "        # TODO: add ops to Tensor\n",
    "        return ((true - pred)**2).mean()\n",
    "\n",
    "criterion = MSELoss()\n",
    "    \n",
    "loss = criterion(pred, y)\n",
    "\n",
    "print(\"loss: \", loss.data)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print ('dL/dw: ', linear.weight.grad) \n",
    "print ('dL/db: ', linear.bias.grad)\n",
    "\n",
    "# optimizer\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, params, lr=0.001):\n",
    "        self.params = params\n",
    "    # todo zero grad function\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param.data -= param.grad * lr\n",
    "            \n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            param.grad = 0\n",
    "    \n",
    "    \n",
    "optimizer = SGD([linear.weight, linear.bias], lr=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce6b472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
