{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a347334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: implement _very_ basic functionalities of pytorch from scratch (but I'm going to cheat and use numpy)\n",
    "\n",
    "# large inspiration from:\n",
    "# - https://github.com/karpathy/micrograd\n",
    "# - https://github.com/geohot/tinygrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91e21485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch \n",
    "# import torchvision\n",
    "# import torch.nn as nn\n",
    "import numpy as np\n",
    "# import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16154db2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9p/ysr3lnbs16z7wxxlztrm7z0m0000gn/T/ipykernel_694/2237398906.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Create tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# pytorch\n",
    "\n",
    "# Create tensors.\n",
    "x = torch.tensor(15., requires_grad=True)\n",
    "print(x)\n",
    "w = torch.tensor(10., requires_grad=True)\n",
    "b = torch.tensor(900., requires_grad=True)\n",
    "\n",
    "# Build a computational graph.\n",
    "y = w * x + b * 100    # y = 2 * x + 3\n",
    "print(y)\n",
    "\n",
    "# Compute gradients.\n",
    "# Computes the sum of gradients of given tensors with respect to graph leaves.\n",
    "y.backward()\n",
    "\n",
    "# Print out the gradients.\n",
    "# Computes and returns the sum of gradients of outputs with respect to the inputs.\n",
    "print(x.grad)    # x.grad = 2 = dy/dx = w\n",
    "print(w.grad)    # w.grad = 1 = dy/dw = x\n",
    "print(b.grad)    # b.grad = 1 = dy/db = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c9f46a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [[15.]]\n",
      "w: [[10.]]\n",
      "b: [[900.]]\n",
      "\n",
      "a  [[10.]] b  [[15.]]\n",
      "y: [[1050.]]\n",
      "a  [[15.]] b  [[1.]]\n",
      "a  [[10.]] b  [[1.]]\n",
      "[[10.]]\n",
      "[[15.]]\n",
      "[[1.]]\n"
     ]
    }
   ],
   "source": [
    "# not pytorch\n",
    "\n",
    "class Tensor:\n",
    "    def __init__(self, data, children=()):\n",
    "        self.data = np.asarray(data, dtype=np.float32)\n",
    "        self.children = children\n",
    "        self.grad = None\n",
    "        self.op = None\n",
    "        \n",
    "    def __mul__(self, other):\n",
    "        op = Multiply\n",
    "        output = op.forward(self, other)\n",
    "        output.op = op\n",
    "        return output\n",
    "    \n",
    "    def matmul(self, other):\n",
    "        op = Matmul\n",
    "        output = op.forward(self, other)\n",
    "        output.op = op\n",
    "        return output\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        op = Add\n",
    "        output = op.forward(self, other)\n",
    "        output.op = op\n",
    "        return output\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        op = Subtract\n",
    "        output = op.forward(self, other)\n",
    "        output.op = op\n",
    "        return output\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        op = Power\n",
    "        output = op.forward(self, other)\n",
    "        output.op = op\n",
    "        return output\n",
    "    \n",
    "    def mean(self):\n",
    "        op = Mean\n",
    "        output = op.forward(self)\n",
    "        output.op = op\n",
    "        return output\n",
    "    \n",
    "    def sum(self):\n",
    "        op = Sum\n",
    "        output = op.forward(self)\n",
    "        output.op = op\n",
    "        return output\n",
    "\n",
    "    def backward(self):\n",
    "        if self.grad is None:\n",
    "            self.grad = Tensor(np.ones(self.data.shape))\n",
    "        if self.op is not None:\n",
    "            children_grads = self.op.backward(self, *self.children)\n",
    "            for node, grad in zip(self.children, children_grads):\n",
    "                node.grad = grad\n",
    "        for node in self.children:\n",
    "            node.backward()\n",
    "            \n",
    "\n",
    "            \n",
    "class Multiply:\n",
    "    def forward(a, b):\n",
    "        print(\"a \", a.data, \"b \", b.data)\n",
    "        return Tensor(np. multiply(a.data, b.data), [a, b])\n",
    "    \n",
    "    def backward(parent, a, b):\n",
    "        \n",
    "        return [b * parent.grad, a * parent.grad]\n",
    "    \n",
    "class Matmul:\n",
    "    def forward(a, b):\n",
    "        print(\"a \", a.data, \"b \", b.data)\n",
    "        return Tensor(np.matmul(a.data, b.data), [a, b])\n",
    "    \n",
    "    def backward(parent, a, b):\n",
    "        \n",
    "        return [b * parent.grad, a * parent.grad]\n",
    "    \n",
    "class Add:\n",
    "    def forward(a, b):\n",
    "        return Tensor(np.add(a.data, b.data), [a, b])\n",
    "    \n",
    "    def backward(parent, a, b):\n",
    "        return [parent.grad, parent.grad]\n",
    "    \n",
    "class Subtract:\n",
    "    def forward(a, b):\n",
    "        return Tensor(np.subtract(a.data, b.data), [a, b])\n",
    "    \n",
    "    def backward(parent, a, b):\n",
    "        return [parent.grad.data, parent.grad.data]\n",
    "    \n",
    "class Power:\n",
    "    def forward(a, b):\n",
    "        return Tensor(np.power(a.data, b.data), [a, b])\n",
    "    \n",
    "    def backward(parent, a, b):\n",
    "        print(\"parent: \", b.data)\n",
    "        return [b * (a**(b-Tensor([[1]]))) * parent.grad, (a**b) * Tensor([[np.log(a.data)]]) * parent.grad]\n",
    "    \n",
    "class Mean:\n",
    "    def forward(a):\n",
    "        return Tensor(np.mean(a.data), [a])\n",
    "    \n",
    "    def backward(parent, a):\n",
    "        if a.grad is None:\n",
    "            a.grad = Tensor(np.ones(a.data.shape))\n",
    "        return [Tensor((1/np.prod(a.data.shape)) * a.grad.data)]\n",
    "    \n",
    "class Sum:\n",
    "    def forward(a):\n",
    "        return Tensor(np.sum(a.data), [a])\n",
    "    \n",
    "    def backward(parent, a):\n",
    "        if a.grad is None:\n",
    "            a.grad = Tensor(np.ones(a.data.shape))\n",
    "        return [a.grad]\n",
    "    \n",
    "        \n",
    "\n",
    "x = Tensor(np.asarray([[15]]))\n",
    "w = Tensor(np.asarray([[10]]))\n",
    "b = Tensor(np.asarray([[900]]))\n",
    "print(f'x: {x.data}')\n",
    "print(f'w: {w.data}')\n",
    "print(f'b: {b.data}')\n",
    "print(f'')\n",
    "\n",
    "y = w.matmul(x) + b\n",
    "print(f'y: {y.data}')\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(x.grad.data)\n",
    "print(w.grad.data)\n",
    "print(b.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4206638f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2.]\n",
      " [3. 6.]]\n",
      "12.0\n",
      "[[1. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "x = Tensor([[1., 2.], [3., 6.]])\n",
    "print(x.data)\n",
    "\n",
    "y = x.sum()\n",
    "print(y.data)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(x.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41ba6618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[3]])\n",
    "b = a@a\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "062ac64d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  [[-0.1367  0.0321]\n",
      " [ 0.4447 -0.2277]]\n",
      "b:  [-0.6949 -0.1522]\n",
      "a  [[-0.7837  0.3945]\n",
      " [-0.1218  0.6905]] b  [[-0.1367  0.0321]\n",
      " [ 0.4447 -0.2277]]\n",
      "pred linear:  [[-0.41233402 -0.2671844 ]\n",
      " [-0.37118456 -0.3133366 ]]\n",
      "loss:  4.0735664\n",
      "parent:  [[2.]]\n",
      "a  [[2.]] b  [[-0.875966   -1.7619156 ]\n",
      " [ 0.43768457 -0.10166338]]\n",
      "a  [[-1.751932   -3.5238311 ]\n",
      " [ 0.87536913 -0.20332676]] b  [[1. 1.]\n",
      " [1. 1.]]\n",
      "a  [[0.76731646 3.1043465 ]\n",
      " [0.19156778 0.01033544]] b  [[[[       nan        nan]\n",
      "   [-0.8262568        nan]]]]\n",
      "a  [[[[        nan         nan]\n",
      "   [-0.15828419         nan]]]] b  [[1. 1.]\n",
      " [1. 1.]]\n",
      "a  [[-0.1367  0.0321]\n",
      " [ 0.4447 -0.2277]] b  <memory at 0x10ca6cee0>\n",
      "a  [[-0.7837  0.3945]\n",
      " [-0.1218  0.6905]] b  <memory at 0x10ca6cee0>\n",
      "dL/dw:  [[ 1.372989   -1.3901514 ]\n",
      " [-0.10661996 -0.14039713]]\n",
      "dL/db:  [[-1.751932   -3.5238311 ]\n",
      " [ 0.87536913 -0.20332676]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9p/ysr3lnbs16z7wxxlztrm7z0m0000gn/T/ipykernel_694/1750195653.py:102: RuntimeWarning: invalid value encountered in log\n",
      "  return [b * (a**(b-Tensor([[1]]))) * parent.grad, (a**b) * Tensor([[np.log(a.data)]]) * parent.grad]\n"
     ]
    }
   ],
   "source": [
    "# TODO: make random\n",
    "x = Tensor([[-0.7837,  0.3945],\n",
    "            [-0.1218,  0.6905]])\n",
    "\n",
    "y = Tensor([[-1.2883, -2.0291],\n",
    "            [ 0.0665, -0.4150]])\n",
    "\n",
    "# nn building blocks\n",
    "# start with linear layer\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        # TODO: make random and transpose weight\n",
    "        self.weight = Tensor(np.transpose([[-0.1367,  0.4447], [ 0.0321, -0.2277]]))\n",
    "        self.bias = Tensor([-0.6949, -0.1522])\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return x.matmul(self.weight) + self.bias\n",
    "    \n",
    "\n",
    "linear = Linear(2, 2)\n",
    "print ('w: ', linear.weight.data)\n",
    "print ('b: ', linear.bias.data)\n",
    "\n",
    "pred = linear(x)\n",
    "print(\"pred linear: \", pred.data)\n",
    "\n",
    "# loss function\n",
    "\n",
    "class MSELoss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, pred, true):\n",
    "        # TODO: add ops to Tensor\n",
    "        return ((true - pred)**Tensor([[2]])).sum()\n",
    "\n",
    "criterion = MSELoss()\n",
    "    \n",
    "loss = criterion(pred, y)\n",
    "\n",
    "print(\"loss: \", loss.data)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print ('dL/dw: ', linear.weight.grad.data) \n",
    "print ('dL/db: ', linear.bias.grad)\n",
    "\n",
    "# optimizer\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, params, lr=0.001):\n",
    "        self.params = params\n",
    "    # todo zero grad function\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param.data -= param.grad * lr\n",
    "            \n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            param.grad = 0\n",
    "    \n",
    "    \n",
    "optimizer = SGD([linear.weight, linear.bias], lr=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74adb9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
