{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170675f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All examples are from: \n",
    "# https://github.com/yunjey/pytorch-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d6e0c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31d33bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred:  tensor([[1.7961]])\n",
      "loss:  tensor(5.6438)\n",
      "dL/dw:  tensor([[4.8977]])\n",
      "dL/db:  tensor([4.7514])\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([[0.3304]], requires_grad=True)\n",
    "b = torch.tensor([0.4247], requires_grad=True)\n",
    "\n",
    "x = torch.tensor([[1.0308]])\n",
    "y = torch.tensor([[-0.5796]])\n",
    "\n",
    "temp =torch.tensor([[1]])\n",
    "\n",
    "y_pred = x.matmul(w.add(temp)).add(b)\n",
    "\n",
    "print(\"pred: \", y_pred.data)\n",
    "\n",
    "l = y.subtract(y_pred).pow(2).mean()\n",
    "\n",
    "print(\"loss: \", l.data)\n",
    "\n",
    "l.backward()\n",
    "\n",
    "print ('dL/dw: ', w.grad.data)\n",
    "print ('dL/db: ', b.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5519ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.)\n",
      "tensor(15.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# ================================================================== #\n",
    "#                     1. Basic autograd example 1                    #\n",
    "# ================================================================== #\n",
    "\n",
    "# Create tensors.\n",
    "x = torch.tensor(15., requires_grad=True)\n",
    "w = torch.tensor(10., requires_grad=True)\n",
    "b = torch.tensor(900., requires_grad=True)\n",
    "\n",
    "# Build a computational graph.\n",
    "y = w * x + b    # y = 2 * x + 3\n",
    "\n",
    "# Compute gradients.\n",
    "# Computes the sum of gradients of given tensors with respect to graph leaves.\n",
    "y.backward()\n",
    "\n",
    "# Print out the gradients.\n",
    "# Computes and returns the sum of gradients of outputs with respect to the inputs.\n",
    "print(x.grad)    # x.grad = 2 = dy/dx = w\n",
    "print(w.grad)    # w.grad = 1 = dy/dw = x\n",
    "print(b.grad)    # b.grad = 1 = dy/db = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2466328a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7837,  0.3945],\n",
      "        [-0.1218,  0.6905]])\n",
      "tensor([[-1.2883, -2.0291],\n",
      "        [ 0.0665, -0.4150]])\n",
      "w:  Parameter containing:\n",
      "tensor([[-0.1367,  0.4447],\n",
      "        [ 0.0321, -0.2277]], requires_grad=True)\n",
      "b:  Parameter containing:\n",
      "tensor([-0.6949, -0.1522], requires_grad=True)\n",
      "pred:  tensor([[-0.4122, -0.2672],\n",
      "        [-0.3711, -0.3134]], grad_fn=<AddmmBackward>)\n",
      "loss:  1.0183411836624146\n",
      "dL/dw:  tensor([[-0.3166,  0.0217],\n",
      "        [-0.6966,  0.3827]])\n",
      "dL/db:  tensor([0.2192, 0.9318])\n",
      "pred:  tensor([[-0.4170, -0.2835],\n",
      "        [-0.3739, -0.3262]], grad_fn=<AddmmBackward>)\n",
      "loss after 1 step optimization:  1.0019701719284058\n"
     ]
    }
   ],
   "source": [
    "# ================================================================== #\n",
    "#                    2. Basic autograd example 2                     #\n",
    "# ================================================================== #\n",
    "\n",
    "# Create tensors of shape (10, 3) and (10, 2).\n",
    "x = torch.randn(2, 2) # inputs\n",
    "y = torch.randn(2, 2) # ouputs\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "# Build a fully connected layer.\n",
    "\n",
    "# Applies a linear transformation to the incoming data: y= xA^T + b\n",
    "linear = nn.Linear(2, 2) # input size = 3, output size = 2\n",
    "print ('w: ', linear.weight) # initial values defined here: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear\n",
    "print ('b: ', linear.bias)\n",
    "\n",
    "# Build loss function and optimizer.\n",
    "\n",
    "# Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input xxx and target yyy.\n",
    "criterion = nn.MSELoss() \n",
    "# Implements stochastic gradient descent (optionally with momentum).\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
    "\n",
    "# Forward pass.\n",
    "pred = linear(x)\n",
    "print ('pred: ', pred) # input shape(10,3), output shape (10,2)\n",
    "\n",
    "# Compute loss.\n",
    "loss = criterion(pred, y) # using formula from docs\n",
    "print('loss: ', loss.item()) # cost\n",
    "\n",
    "# Backward pass.\n",
    "loss.backward() # run backward pass on output to compute gradients across linear layer\n",
    "\n",
    "# Print out the gradients.\n",
    "# calucated from backward pass\n",
    "print ('dL/dw: ', linear.weight.grad) \n",
    "print ('dL/db: ', linear.bias.grad)\n",
    "\n",
    "# 1-step gradient descent.\n",
    "optimizer.step()\n",
    "\n",
    "# You can also perform gradient descent at the low level.\n",
    "# linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n",
    "# linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n",
    "\n",
    "# Print out the loss after 1-step gradient descent.\n",
    "pred = linear(x)\n",
    "print(\"pred: \", pred)\n",
    "loss = criterion(pred, y)\n",
    "print('loss after 1 step optimization: ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "681fc8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.41233406 -0.26718442]\n",
      " [-0.37118459 -0.31333663]]\n",
      "1.0183915200525115\n",
      "1.0183915200525115\n"
     ]
    }
   ],
   "source": [
    "x = np.asarray([[-0.7837,  0.3945],\n",
    "        [-0.1218,  0.6905]])\n",
    "y = np.asarray([[-1.2883, -2.0291],\n",
    "        [ 0.0665, -0.4150]])\n",
    "\n",
    "A = np.asarray([[-0.1367,  0.4447],\n",
    "        [ 0.0321, -0.2277]])\n",
    "b = np.asarray([-0.6949, -0.1522])\n",
    "\n",
    "y_pred = x@np.transpose(A)+b # matmal\n",
    "print(y_pred)\n",
    "\n",
    "loss = ((y-y_pred)**2).mean()\n",
    "print(loss)\n",
    "\n",
    "mat = (y-y_pred)**2  # element-wise\n",
    "# mean = (mat[0,0] + mat[0,1] + mat[1,0] + mat[1,1])/4\n",
    "mean = mat.sum()/4\n",
    "\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a274c43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12., grad_fn=<SumBackward0>)\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "b = 6\n",
    "x = torch.tensor([[1, 2.], [3., 6.]], requires_grad=True)\n",
    "\n",
    "# y = x.matmul(x).sum()\n",
    "y = x.sum()\n",
    "print(y)\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8f6e8958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.,  0., -2.]], grad_fn=<MmBackward>)\n",
      "tensor([[ 2.,  2.,  2.],\n",
      "        [ 0.,  0.,  0.],\n",
      "        [-2., -2., -2.]])\n",
      "tensor([[1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.eye(3, requires_grad=True)\n",
    "y = torch.tensor([[2.0,0,-2.0]], requires_grad=True)\n",
    "z = y.matmul(x).sum()\n",
    "z.backward()\n",
    "\n",
    "print(x.grad)  # dz/dx\n",
    "print(y.grad)  # dz/dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87c38bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]] tensor([[1, 2],\n",
      "        [3, 4]]) [[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "# ================================================================== #\n",
    "#                     3. Loading data from numpy                     #\n",
    "# ================================================================== #\n",
    "\n",
    "# Create a numpy array.\n",
    "x = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "# Convert the numpy array to a torch tensor.\n",
    "y = torch.from_numpy(x)\n",
    "\n",
    "# Convert the torch tensor to a numpy array.\n",
    "z = y.numpy()\n",
    "\n",
    "# just some conversions\n",
    "print(x, y, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "191b28d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../../data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.8%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "5.8%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "9.5%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "13.5%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "17.4%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "21.3%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================================================== #\n",
    "#                         4. Input pipeline                           #\n",
    "# ================================================================== #\n",
    "\n",
    "# Download and construct CIFAR-10 dataset.\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
    "                                             train=True, \n",
    "                                             transform=transforms.ToTensor(),\n",
    "                                             download=True)\n",
    "\n",
    "# Fetch one data pair (read data from disk).\n",
    "image, label = train_dataset[0]\n",
    "print (image.size())\n",
    "print (label)\n",
    "\n",
    "# Data loader (this provides queues and threads in a very simple way).\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True)\n",
    "\n",
    "# When iteration starts, queue and thread start to load data from files.\n",
    "data_iter = iter(train_loader)\n",
    "\n",
    "# Mini-batch images and labels.\n",
    "images, labels = data_iter.next()\n",
    "\n",
    "# Actual usage of the data loader is as below.\n",
    "for images, labels in train_loader:\n",
    "    # Training code should be written here.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "951d01a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9p/ysr3lnbs16z7wxxlztrm7z0m0000gn/T/ipykernel_81730/3838804325.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# You can then use the prebuilt data loader.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mcustom_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m train_loader = torch.utils.data.DataLoader(dataset=custom_dataset,\n\u001b[0m\u001b[1;32m     24\u001b[0m                                            \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                                            shuffle=True)\n",
      "\u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers)\u001b[0m\n\u001b[1;32m    268\u001b[0m                     \u001b[0;31m# Cannot statically verify that dataset is Sized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m                     \u001b[0;31m# Somewhat related: see NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.8/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[0m\u001b[1;32m    103\u001b[0m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "# ================================================================== #\n",
    "#                5. Input pipeline for custom dataset                 #\n",
    "# ================================================================== #\n",
    "\n",
    "# You should build your custom dataset as below.\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        # TODO\n",
    "        # 1. Initialize file paths or a list of file names. \n",
    "        pass\n",
    "    def __getitem__(self, index):\n",
    "        # TODO\n",
    "        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n",
    "        # 2. Preprocess the data (e.g. torchvision.Transform).\n",
    "        # 3. Return a data pair (e.g. image and label).\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        # You should change 0 to the total size of your dataset.\n",
    "        return 0 \n",
    "\n",
    "# You can then use the prebuilt data loader. \n",
    "custom_dataset = CustomDataset()\n",
    "train_loader = torch.utils.data.DataLoader(dataset=custom_dataset,\n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79eb0960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /Users/willmartin/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100.0%\n",
      "/Users/willmartin/miniforge3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100])\n"
     ]
    }
   ],
   "source": [
    "# ================================================================== #\n",
    "#                        6. Pretrained model                         #\n",
    "# ================================================================== #\n",
    "\n",
    "# Download and load the pretrained ResNet-18.\n",
    "resnet = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# If you want to finetune only the top layer of the model, set as below.\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the top layer for finetuning.\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, 100)  # 100 is an example.\n",
    "\n",
    "# Forward pass.\n",
    "images = torch.randn(64, 3, 224, 224)\n",
    "outputs = resnet(images)\n",
    "print (outputs.size())     # (64, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f60c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================== #\n",
    "#                      7. Save and load the model                    #\n",
    "# ================================================================== #\n",
    "\n",
    "# Save and load the entire model.\n",
    "torch.save(resnet, 'model.ckpt')\n",
    "model = torch.load('model.ckpt')\n",
    "\n",
    "# Save and load only the model parameters (recommended).\n",
    "torch.save(resnet.state_dict(), 'params.ckpt')\n",
    "resnet.load_state_dict(torch.load('params.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0115ba5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/60], Loss: 11.8994\n",
      "Epoch [10/60], Loss: 4.9264\n",
      "Epoch [15/60], Loss: 2.1016\n",
      "Epoch [20/60], Loss: 0.9571\n",
      "Epoch [25/60], Loss: 0.4935\n",
      "Epoch [30/60], Loss: 0.3057\n",
      "Epoch [35/60], Loss: 0.2296\n",
      "Epoch [40/60], Loss: 0.1987\n",
      "Epoch [45/60], Loss: 0.1862\n",
      "Epoch [50/60], Loss: 0.1811\n",
      "Epoch [55/60], Loss: 0.1791\n",
      "Epoch [60/60], Loss: 0.1782\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# linear regression ->  continuous values\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Hyper-parameters\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "num_epochs = 60\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Toy dataset\n",
    "x_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168], \n",
    "                    [9.779], [6.182], [7.59], [2.167], [7.042], \n",
    "                    [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)\n",
    "\n",
    "y_train = np.array([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573], \n",
    "                    [3.366], [2.596], [2.53], [1.221], [2.827], \n",
    "                    [3.465], [1.65], [2.904], [1.3]], dtype=np.float32)\n",
    "\n",
    "# Linear regression model\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Convert numpy arrays to torch tensors\n",
    "    inputs = torch.from_numpy(x_train)\n",
    "    targets = torch.from_numpy(y_train)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "# Plot the graph\n",
    "predicted = model(torch.from_numpy(x_train)).detach().numpy()\n",
    "plt.plot(x_train, y_train, 'ro', label='Original data')\n",
    "plt.plot(x_train, predicted, label='Fitted line')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c40edc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Step [100/600], Loss: 2.2115\n",
      "Epoch [1/25], Step [200/600], Loss: 2.0691\n",
      "Epoch [1/25], Step [300/600], Loss: 1.9880\n",
      "Epoch [1/25], Step [400/600], Loss: 1.9090\n",
      "Epoch [1/25], Step [500/600], Loss: 1.8426\n",
      "Epoch [1/25], Step [600/600], Loss: 1.8276\n",
      "Epoch [2/25], Step [100/600], Loss: 1.7279\n",
      "Epoch [2/25], Step [200/600], Loss: 1.6645\n",
      "Epoch [2/25], Step [300/600], Loss: 1.5318\n",
      "Epoch [2/25], Step [400/600], Loss: 1.4984\n",
      "Epoch [2/25], Step [500/600], Loss: 1.5609\n",
      "Epoch [2/25], Step [600/600], Loss: 1.3909\n",
      "Epoch [3/25], Step [100/600], Loss: 1.4090\n",
      "Epoch [3/25], Step [200/600], Loss: 1.3445\n",
      "Epoch [3/25], Step [300/600], Loss: 1.3562\n",
      "Epoch [3/25], Step [400/600], Loss: 1.2851\n",
      "Epoch [3/25], Step [500/600], Loss: 1.2856\n",
      "Epoch [3/25], Step [600/600], Loss: 1.2732\n",
      "Epoch [4/25], Step [100/600], Loss: 1.1928\n",
      "Epoch [4/25], Step [200/600], Loss: 1.2317\n",
      "Epoch [4/25], Step [300/600], Loss: 1.1752\n",
      "Epoch [4/25], Step [400/600], Loss: 1.1073\n",
      "Epoch [4/25], Step [500/600], Loss: 1.1063\n",
      "Epoch [4/25], Step [600/600], Loss: 1.0856\n",
      "Epoch [5/25], Step [100/600], Loss: 1.0696\n",
      "Epoch [5/25], Step [200/600], Loss: 1.0257\n",
      "Epoch [5/25], Step [300/600], Loss: 1.0969\n",
      "Epoch [5/25], Step [400/600], Loss: 0.9613\n",
      "Epoch [5/25], Step [500/600], Loss: 1.0076\n",
      "Epoch [5/25], Step [600/600], Loss: 0.9809\n",
      "Epoch [6/25], Step [100/600], Loss: 0.9419\n",
      "Epoch [6/25], Step [200/600], Loss: 0.9865\n",
      "Epoch [6/25], Step [300/600], Loss: 0.8945\n",
      "Epoch [6/25], Step [400/600], Loss: 1.0006\n",
      "Epoch [6/25], Step [500/600], Loss: 0.8482\n",
      "Epoch [6/25], Step [600/600], Loss: 0.9636\n",
      "Epoch [7/25], Step [100/600], Loss: 0.9248\n",
      "Epoch [7/25], Step [200/600], Loss: 0.9047\n",
      "Epoch [7/25], Step [300/600], Loss: 0.8926\n",
      "Epoch [7/25], Step [400/600], Loss: 0.8274\n",
      "Epoch [7/25], Step [500/600], Loss: 0.7680\n",
      "Epoch [7/25], Step [600/600], Loss: 0.7154\n",
      "Epoch [8/25], Step [100/600], Loss: 0.8818\n",
      "Epoch [8/25], Step [200/600], Loss: 0.7614\n",
      "Epoch [8/25], Step [300/600], Loss: 0.9351\n",
      "Epoch [8/25], Step [400/600], Loss: 0.7321\n",
      "Epoch [8/25], Step [500/600], Loss: 0.8212\n",
      "Epoch [8/25], Step [600/600], Loss: 0.7040\n",
      "Epoch [9/25], Step [100/600], Loss: 0.8134\n",
      "Epoch [9/25], Step [200/600], Loss: 0.7989\n",
      "Epoch [9/25], Step [300/600], Loss: 0.6984\n",
      "Epoch [9/25], Step [400/600], Loss: 0.8062\n",
      "Epoch [9/25], Step [500/600], Loss: 0.8258\n",
      "Epoch [9/25], Step [600/600], Loss: 0.7793\n",
      "Epoch [10/25], Step [100/600], Loss: 0.7492\n",
      "Epoch [10/25], Step [200/600], Loss: 0.7144\n",
      "Epoch [10/25], Step [300/600], Loss: 0.7186\n",
      "Epoch [10/25], Step [400/600], Loss: 0.7195\n",
      "Epoch [10/25], Step [500/600], Loss: 0.9002\n",
      "Epoch [10/25], Step [600/600], Loss: 0.6525\n",
      "Epoch [11/25], Step [100/600], Loss: 0.6738\n",
      "Epoch [11/25], Step [200/600], Loss: 0.7284\n",
      "Epoch [11/25], Step [300/600], Loss: 0.6766\n",
      "Epoch [11/25], Step [400/600], Loss: 0.6807\n",
      "Epoch [11/25], Step [500/600], Loss: 0.6180\n",
      "Epoch [11/25], Step [600/600], Loss: 0.8427\n",
      "Epoch [12/25], Step [100/600], Loss: 0.6956\n",
      "Epoch [12/25], Step [200/600], Loss: 0.6521\n",
      "Epoch [12/25], Step [300/600], Loss: 0.7070\n",
      "Epoch [12/25], Step [400/600], Loss: 0.5940\n",
      "Epoch [12/25], Step [500/600], Loss: 0.7066\n",
      "Epoch [12/25], Step [600/600], Loss: 0.5850\n",
      "Epoch [13/25], Step [100/600], Loss: 0.6466\n",
      "Epoch [13/25], Step [200/600], Loss: 0.6342\n",
      "Epoch [13/25], Step [300/600], Loss: 0.6499\n",
      "Epoch [13/25], Step [400/600], Loss: 0.5398\n",
      "Epoch [13/25], Step [500/600], Loss: 0.6161\n",
      "Epoch [13/25], Step [600/600], Loss: 0.8598\n",
      "Epoch [14/25], Step [100/600], Loss: 0.7916\n",
      "Epoch [14/25], Step [200/600], Loss: 0.6849\n",
      "Epoch [14/25], Step [300/600], Loss: 0.6671\n",
      "Epoch [14/25], Step [400/600], Loss: 0.6089\n",
      "Epoch [14/25], Step [500/600], Loss: 0.6538\n",
      "Epoch [14/25], Step [600/600], Loss: 0.6972\n",
      "Epoch [15/25], Step [100/600], Loss: 0.6039\n",
      "Epoch [15/25], Step [200/600], Loss: 0.7263\n",
      "Epoch [15/25], Step [300/600], Loss: 0.6242\n",
      "Epoch [15/25], Step [400/600], Loss: 0.6976\n",
      "Epoch [15/25], Step [500/600], Loss: 0.6697\n",
      "Epoch [15/25], Step [600/600], Loss: 0.7747\n",
      "Epoch [16/25], Step [100/600], Loss: 0.6930\n",
      "Epoch [16/25], Step [200/600], Loss: 0.6582\n",
      "Epoch [16/25], Step [300/600], Loss: 0.5793\n",
      "Epoch [16/25], Step [400/600], Loss: 0.5933\n",
      "Epoch [16/25], Step [500/600], Loss: 0.6080\n",
      "Epoch [16/25], Step [600/600], Loss: 0.5928\n",
      "Epoch [17/25], Step [100/600], Loss: 0.5498\n",
      "Epoch [17/25], Step [200/600], Loss: 0.5707\n",
      "Epoch [17/25], Step [300/600], Loss: 0.6356\n",
      "Epoch [17/25], Step [400/600], Loss: 0.7379\n",
      "Epoch [17/25], Step [500/600], Loss: 0.6083\n",
      "Epoch [17/25], Step [600/600], Loss: 0.6177\n",
      "Epoch [18/25], Step [100/600], Loss: 0.5452\n",
      "Epoch [18/25], Step [200/600], Loss: 0.5807\n",
      "Epoch [18/25], Step [300/600], Loss: 0.6145\n",
      "Epoch [18/25], Step [400/600], Loss: 0.5960\n",
      "Epoch [18/25], Step [500/600], Loss: 0.7265\n",
      "Epoch [18/25], Step [600/600], Loss: 0.6489\n",
      "Epoch [19/25], Step [100/600], Loss: 0.6791\n",
      "Epoch [19/25], Step [200/600], Loss: 0.5166\n",
      "Epoch [19/25], Step [300/600], Loss: 0.6619\n",
      "Epoch [19/25], Step [400/600], Loss: 0.6335\n",
      "Epoch [19/25], Step [500/600], Loss: 0.5770\n",
      "Epoch [19/25], Step [600/600], Loss: 0.6263\n",
      "Epoch [20/25], Step [100/600], Loss: 0.4718\n",
      "Epoch [20/25], Step [200/600], Loss: 0.6112\n",
      "Epoch [20/25], Step [300/600], Loss: 0.5904\n",
      "Epoch [20/25], Step [400/600], Loss: 0.6012\n",
      "Epoch [20/25], Step [500/600], Loss: 0.5885\n",
      "Epoch [20/25], Step [600/600], Loss: 0.5022\n",
      "Epoch [21/25], Step [100/600], Loss: 0.5777\n",
      "Epoch [21/25], Step [200/600], Loss: 0.5819\n",
      "Epoch [21/25], Step [300/600], Loss: 0.5880\n",
      "Epoch [21/25], Step [400/600], Loss: 0.6235\n",
      "Epoch [21/25], Step [500/600], Loss: 0.5413\n",
      "Epoch [21/25], Step [600/600], Loss: 0.4291\n",
      "Epoch [22/25], Step [100/600], Loss: 0.6145\n",
      "Epoch [22/25], Step [200/600], Loss: 0.5421\n",
      "Epoch [22/25], Step [300/600], Loss: 0.6124\n",
      "Epoch [22/25], Step [400/600], Loss: 0.4351\n",
      "Epoch [22/25], Step [500/600], Loss: 0.6389\n",
      "Epoch [22/25], Step [600/600], Loss: 0.5983\n",
      "Epoch [23/25], Step [100/600], Loss: 0.4749\n",
      "Epoch [23/25], Step [200/600], Loss: 0.5491\n",
      "Epoch [23/25], Step [300/600], Loss: 0.5291\n",
      "Epoch [23/25], Step [400/600], Loss: 0.4463\n",
      "Epoch [23/25], Step [500/600], Loss: 0.5344\n",
      "Epoch [23/25], Step [600/600], Loss: 0.5553\n",
      "Epoch [24/25], Step [100/600], Loss: 0.6861\n",
      "Epoch [24/25], Step [200/600], Loss: 0.5605\n",
      "Epoch [24/25], Step [300/600], Loss: 0.4941\n",
      "Epoch [24/25], Step [400/600], Loss: 0.6090\n",
      "Epoch [24/25], Step [500/600], Loss: 0.5708\n",
      "Epoch [24/25], Step [600/600], Loss: 0.5512\n",
      "Epoch [25/25], Step [100/600], Loss: 0.5203\n",
      "Epoch [25/25], Step [200/600], Loss: 0.6658\n",
      "Epoch [25/25], Step [300/600], Loss: 0.5797\n",
      "Epoch [25/25], Step [400/600], Loss: 0.6221\n",
      "Epoch [25/25], Step [500/600], Loss: 0.3966\n",
      "Epoch [25/25], Step [600/600], Loss: 0.6087\n",
      "Accuracy of the model on the 10000 test images: 88.05000305175781 %\n"
     ]
    }
   ],
   "source": [
    "# logistic regression -> classification\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Hyper-parameters \n",
    "input_size = 28 * 28    # 784\n",
    "num_classes = 10\n",
    "num_epochs = 25\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset (images and labels)\n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader (input pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# Logistic regression model\n",
    "model = nn.Linear(input_size, num_classes)\n",
    "\n",
    "# Loss and optimizer\n",
    "# nn.CrossEntropyLoss() computes softmax internally\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Reshape images to (batch_size, input_size)\n",
    "        images = images.reshape(-1, input_size)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, input_size)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "\n",
    "    print('Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f51df933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/600], Loss: 0.3254\n",
      "Epoch [1/10], Step [200/600], Loss: 0.2177\n",
      "Epoch [1/10], Step [300/600], Loss: 0.2197\n",
      "Epoch [1/10], Step [400/600], Loss: 0.1206\n",
      "Epoch [1/10], Step [500/600], Loss: 0.1867\n",
      "Epoch [1/10], Step [600/600], Loss: 0.1071\n",
      "Epoch [2/10], Step [100/600], Loss: 0.2689\n",
      "Epoch [2/10], Step [200/600], Loss: 0.1360\n",
      "Epoch [2/10], Step [300/600], Loss: 0.1419\n",
      "Epoch [2/10], Step [400/600], Loss: 0.0594\n",
      "Epoch [2/10], Step [500/600], Loss: 0.0816\n",
      "Epoch [2/10], Step [600/600], Loss: 0.1443\n",
      "Epoch [3/10], Step [100/600], Loss: 0.0324\n",
      "Epoch [3/10], Step [200/600], Loss: 0.0768\n",
      "Epoch [3/10], Step [300/600], Loss: 0.1642\n",
      "Epoch [3/10], Step [400/600], Loss: 0.0456\n",
      "Epoch [3/10], Step [500/600], Loss: 0.0232\n",
      "Epoch [3/10], Step [600/600], Loss: 0.1513\n",
      "Epoch [4/10], Step [100/600], Loss: 0.0125\n",
      "Epoch [4/10], Step [200/600], Loss: 0.0782\n",
      "Epoch [4/10], Step [300/600], Loss: 0.0173\n",
      "Epoch [4/10], Step [400/600], Loss: 0.0515\n",
      "Epoch [4/10], Step [500/600], Loss: 0.0676\n",
      "Epoch [4/10], Step [600/600], Loss: 0.0279\n",
      "Epoch [5/10], Step [100/600], Loss: 0.0556\n",
      "Epoch [5/10], Step [200/600], Loss: 0.0526\n",
      "Epoch [5/10], Step [300/600], Loss: 0.0306\n",
      "Epoch [5/10], Step [400/600], Loss: 0.0484\n",
      "Epoch [5/10], Step [500/600], Loss: 0.0347\n",
      "Epoch [5/10], Step [600/600], Loss: 0.0111\n",
      "Epoch [6/10], Step [100/600], Loss: 0.0117\n",
      "Epoch [6/10], Step [200/600], Loss: 0.0330\n",
      "Epoch [6/10], Step [300/600], Loss: 0.0074\n",
      "Epoch [6/10], Step [400/600], Loss: 0.0304\n",
      "Epoch [6/10], Step [500/600], Loss: 0.0173\n",
      "Epoch [6/10], Step [600/600], Loss: 0.0272\n",
      "Epoch [7/10], Step [100/600], Loss: 0.0061\n",
      "Epoch [7/10], Step [200/600], Loss: 0.0454\n",
      "Epoch [7/10], Step [300/600], Loss: 0.0135\n",
      "Epoch [7/10], Step [400/600], Loss: 0.0312\n",
      "Epoch [7/10], Step [500/600], Loss: 0.0197\n",
      "Epoch [7/10], Step [600/600], Loss: 0.0584\n",
      "Epoch [8/10], Step [100/600], Loss: 0.0080\n",
      "Epoch [8/10], Step [200/600], Loss: 0.0586\n",
      "Epoch [8/10], Step [300/600], Loss: 0.0238\n",
      "Epoch [8/10], Step [400/600], Loss: 0.0146\n",
      "Epoch [8/10], Step [500/600], Loss: 0.0222\n",
      "Epoch [8/10], Step [600/600], Loss: 0.0105\n",
      "Epoch [9/10], Step [100/600], Loss: 0.0128\n",
      "Epoch [9/10], Step [200/600], Loss: 0.0085\n",
      "Epoch [9/10], Step [300/600], Loss: 0.0265\n",
      "Epoch [9/10], Step [400/600], Loss: 0.0032\n",
      "Epoch [9/10], Step [500/600], Loss: 0.0160\n",
      "Epoch [9/10], Step [600/600], Loss: 0.0016\n",
      "Epoch [10/10], Step [100/600], Loss: 0.0050\n",
      "Epoch [10/10], Step [200/600], Loss: 0.0013\n",
      "Epoch [10/10], Step [300/600], Loss: 0.0177\n",
      "Epoch [10/10], Step [400/600], Loss: 0.0022\n",
      "Epoch [10/10], Step [500/600], Loss: 0.0193\n",
      "Epoch [10/10], Step [600/600], Loss: 0.0058\n",
      "Accuracy of the network on the 10000 test images: 98.03 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters \n",
    "input_size = 784\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ae736c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/willmartin/miniforge3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ../c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.1019\n",
      "Epoch [1/5], Step [200/600], Loss: 0.1496\n",
      "Epoch [1/5], Step [300/600], Loss: 0.1169\n",
      "Epoch [1/5], Step [400/600], Loss: 0.0505\n",
      "Epoch [1/5], Step [500/600], Loss: 0.1541\n",
      "Epoch [1/5], Step [600/600], Loss: 0.0389\n",
      "Epoch [2/5], Step [100/600], Loss: 0.0131\n",
      "Epoch [2/5], Step [200/600], Loss: 0.0161\n",
      "Epoch [2/5], Step [300/600], Loss: 0.0392\n",
      "Epoch [2/5], Step [400/600], Loss: 0.1538\n",
      "Epoch [2/5], Step [500/600], Loss: 0.0746\n",
      "Epoch [2/5], Step [600/600], Loss: 0.0678\n",
      "Epoch [3/5], Step [100/600], Loss: 0.0638\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0124\n",
      "Epoch [3/5], Step [300/600], Loss: 0.0537\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0197\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0625\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0482\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0298\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0190\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0483\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0207\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0838\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0552\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0272\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0566\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0086\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0298\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0048\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0722\n",
      "Test Accuracy of the model on the 10000 test images: 98.88 %\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper parameters\n",
    "num_epochs = 5\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNet(num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5273971b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../../data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.8%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "5.9%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "9.6%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "13.3%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "16.8%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9p/ysr3lnbs16z7wxxlztrm7z0m0000gn/T/ipykernel_82188/1140823555.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# CIFAR-10 dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n\u001b[0m\u001b[1;32m     31\u001b[0m                                              \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                                              \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.8/site-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.8/site-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Files already downloaded and verified'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mdownload_and_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgz_md5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.8/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m     \u001b[0mdownload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.8/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Downloading '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0m_urlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mURLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'https'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.8/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36m_urlretrieve\u001b[0;34m(url, filename, chunk_size)\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                     \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                     \u001b[0mfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.8/site-packages/torch/hub.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\r{0:.1f}%\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.8/site-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                 \u001b[0;31m# and give a timeout to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m                     \u001b[0;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                     \u001b[0;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------- #\n",
    "# An implementation of https://arxiv.org/pdf/1512.03385.pdf                    #\n",
    "# See section 4.2 for the model architecture on CIFAR-10                       #\n",
    "# Some part of the code was referenced from below                              #\n",
    "# https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py   #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "num_epochs = 80\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Image preprocessing modules\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "# CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
    "                                             train=True, \n",
    "                                             transform=transform,\n",
    "                                             download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
    "                                            train=False, \n",
    "                                            transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "# 3x3 convolution\n",
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                     stride=stride, padding=1, bias=False)\n",
    "\n",
    "# Residual block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# ResNet\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 16\n",
    "        self.conv = conv3x3(3, 16)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self.make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self.make_layer(block, 32, layers[1], 2)\n",
    "        self.layer3 = self.make_layer(block, 64, layers[2], 2)\n",
    "        self.avg_pool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "        \n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if (stride != 1) or (self.in_channels != out_channels):\n",
    "            downsample = nn.Sequential(\n",
    "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "model = ResNet(ResidualBlock, [2, 2, 2]).to(device)\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# For updating learning rate\n",
    "def update_lr(optimizer, lr):    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "curr_lr = learning_rate\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "    # Decay learning rate\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        curr_lr /= 3\n",
    "        update_lr(optimizer, curr_lr)\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'resnet.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
